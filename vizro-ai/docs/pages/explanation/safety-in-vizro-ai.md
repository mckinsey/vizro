## Warning and safety usage for generative AI models

Vizro-AI uses generative AI models because large language models (LLMs) represent significant advancements in the AI field. However, as with any powerful tool, there are potential risks associated with connecting to a generative AI model.

We recommend users research and understand the selected model before using `vizro_ai` package.

Users are encouraged to treat AI-generated content as supplementary, **always apply human judgment**, approach with caution, review the relevant [disclaimer](disclaimer.md) page, and consider the following:

<!-- vale off -->

### 1. Hallucination and misrepresentation

Generative models can potentially generate information while appearing factual, being entirely fictitious or misleading.

<!-- vale on -->

The vendor models might lack real-time knowledge or events beyond its last updates. `vizro_ai` output may vary and you should always verify critical information. It is the user's responsibility to discern the accuracy, consistent, and reliability of the generated content.

<!-- vale off -->

### 2. Unintended and sensitive output

<!-- vale on -->

The outputs from these models can be unexpected, inappropriate, or even harmful. Users as human in the loop is an essential part. Users must check and interpret the final output. It is necessary to approach the generated content with caution, especially when shared or applied in various contexts.

<!-- vale off -->

### 3. Data privacy

<!-- vale on -->

Your data is sent to model vendors if you connect to LLMs via their APIs. For example, if you connect to the model from OpenAI, your data will be sent to OpenAI via their API. Users should be cautious about sharing or inputting any personal or sensitive information.

<!-- vale off -->

### 4. Bias and fairness

<!-- vale on -->

Generative AI can exhibit biases present in their training data. Users need to be aware of and navigate potential biases in generated outputs and be cautious when interpreting the generated content.

<!-- vale off -->

### 5. Malicious use

<!-- vale on -->

These models can be exploited for various malicious activities. Users should be cautious about how and where they deploy and access such models.

It's crucial for users to remain informed, cautious, and ethical in their applications.

## Dependencies, code scanners, and information security

It may occur that dependencies of `vizro_ai` get flagged by code scanners and other information security tools. As a consequence it may happen that `vizro_ai` also get flagged.

While we aim to resolve any flagged issues as soon as possible, there may not always be an immediate available fix, especially in a dynamic environment such as generative AI. We encourage users to investigate if any flagged information security issues are actually related to any functionality used in our code base or if they only concern functionality outside the scope of `vizro_ai`.

In case those issues are related to code execution, note that `vizro_ai` has its own process of executing dynamic code (see [Safeguard Execution of Dynamic Code](safeguard.md)), and does not rely on its dependencies to do so.

## Execution of dynamic code in Vizro-AI

The `exec()` statement is used in `vizro_ai`. It enables dynamic execution of Python programs which can be powerful, but can also pose security risk if used without caution. When paired with outputs from generative models, there is potential for unintended or malicious code execution.

Users must exercise caution when executing code generated by or influenced by AI models. It's essential to:

- Never input prompts designed to generate harmful or malicious code.
- Always consider using controlled environments, such as virtual machines or containers, to execute uncertain code
- Always be aware of potential risks when executing dynamically generated code in environments with access to sensitive data or systems
- Always be aware that malicious code execution cannot be mitigated with 100% certainty
- Always review and understand the selected model before connecting with `vizro_ai`

To learn more, refer to the section that describes how to [safeguard execution of dynamic code](safeguard.md).

## Debugging in Vizro-AI with LangSmith and LangFuse

[LangSmith](https://docs.smith.langchain.com/) and [LangFuse](https://langfuse.com/docs) are tools designed to enhance transparency and interpretability in AI workflows. To improve debugging and traceability, you can use the advanced observability and evaluation features of these tools with Vizro-AI.

To ensure responsible use, review their data privacy and security policies. See [LangFuse Data Security & Privacy ](https://langfuse.com/docs/data-security-privacy) and [LangSmith Privacy Policy](https://www.langchain.com/privacy-policy) site for more details.

Also, both support self-hosting options, which may be critical for security oriented users.See [LangSmith](https://docs.smith.langchain.com/self_hosting) and [LangFuse](https://langfuse.com/docs/deployment/self-host) self-hosting guides for more details. LangFuse offers a self-hosting service under the MIT License.
